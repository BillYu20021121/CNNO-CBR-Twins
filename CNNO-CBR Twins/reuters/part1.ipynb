{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import reuters\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Conv1D, MaxPooling1D, Flatten, Dropout, Activation, Layer, SimpleRNN\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8982, 500)\n",
      "8982 train samples\n",
      "2246 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bill\\.conda\\envs\\python3.6\\lib\\site-packages\\keras\\datasets\\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "c:\\Users\\Bill\\.conda\\envs\\python3.6\\lib\\site-packages\\keras\\datasets\\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "# Load the reuters dataset\n",
    "max_features = 5000  # Number of words to consider as features\n",
    "max_len = 500  # Cut texts after this number of words (among top max_features most common words)\n",
    "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=max_features)\n",
    "\n",
    "# Pad sequences (to ensure equal length of all sequences)\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = max(y_train) + 1\n",
    "oh_y_train = to_categorical(y_train, num_classes)\n",
    "oh_y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeeOscillator:\n",
    "    def __init__(self, a=[1, 1, 1, 1, -1, -1, -1, -1], b=[0.6, 0.6, -0.5, 0.5, -0.6, -0.6, -0.5, 0.5], K=500, N=100):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "\n",
    "    def _initialize_tensors(self, x, N):\n",
    "        shape = tf.shape(x)\n",
    "        rank = tf.rank(x)\n",
    "\n",
    "        if rank == 2:\n",
    "            x = tf.expand_dims(tf.expand_dims(x, axis=1), axis=1)\n",
    "            shape = tf.shape(x)\n",
    "        elif rank == 4 and shape[1] > 1:\n",
    "            x = tf.transpose(x, [0, 2, 3, 1])\n",
    "            shape = tf.shape(x)\n",
    "\n",
    "        init_shape = tf.concat([[N], shape], axis=0)\n",
    "        u = tf.zeros(init_shape, dtype=tf.float32)\n",
    "        v = tf.zeros(init_shape, dtype=tf.float32)\n",
    "        z = tf.zeros(init_shape, dtype=tf.float32)\n",
    "        u = tf.tensor_scatter_nd_update(u, [[0]], [u[0] + 0.2])\n",
    "        z = tf.tensor_scatter_nd_update(z, [[0]], [z[0] + 0.2])\n",
    "\n",
    "        return u, v, z, x\n",
    "\n",
    "    def Tanh(self, x):\n",
    "        N = np.random.randint(1, self.N + 1)\n",
    "        u, v, z, x = self._initialize_tensors(x, N)\n",
    "\n",
    "        for t in range(N - 1):\n",
    "            u = tf.tensor_scatter_nd_update(u, [[t + 1]], [\n",
    "                tf.math.tanh(self.a[0] * u[t] - self.a[1] * v[t] + self.a[2] * z[t] + self.a[3] * x)])\n",
    "            v = tf.tensor_scatter_nd_update(v, [[t + 1]], [\n",
    "                tf.math.tanh(self.a[6] * z[t] - self.a[4] * u[t] - self.a[5] * v[t] + self.a[7] * x)])\n",
    "            w = tf.math.tanh(x)\n",
    "            z = tf.tensor_scatter_nd_update(z, [[t + 1]], [(v[t + 1] - u[t + 1]) * tf.math.exp(-self.K * tf.math.pow(x, 2)) + w])\n",
    "\n",
    "        if tf.rank(x) == 4 and tf.shape(x)[1] > 1:\n",
    "            z = tf.transpose(z[-1], [0, 3, 1, 2])\n",
    "\n",
    "        return z[-1]\n",
    "\n",
    "    def Sigmoid(self, x):\n",
    "        N = np.random.randint(1, self.N + 1)\n",
    "        u, v, z, x = self._initialize_tensors(x, N)\n",
    "\n",
    "        for t in range(N - 1):\n",
    "            u = tf.tensor_scatter_nd_update(u, [[t + 1]], [\n",
    "                tf.math.sigmoid(self.b[0] * u[t] - self.b[1] * v[t] + self.b[2] * z[t] + self.b[3] * x)])\n",
    "            v = tf.tensor_scatter_nd_update(v, [[t + 1]], [\n",
    "                tf.math.sigmoid(self.b[6] * z[t] - self.b[4] * u[t] - self.b[5] * v[t] + self.b[7] * x)])\n",
    "            w = tf.math.sigmoid(x)\n",
    "            z = tf.tensor_scatter_nd_update(z, [[t + 1]], [(v[t + 1] - u[t + 1]) * tf.math.exp(-self.K * tf.math.pow(x, 2)) + w])\n",
    "\n",
    "        if tf.rank(x) == 4 and tf.shape(x)[1] > 1:\n",
    "            z = tf.transpose(z[-1], [0, 3, 1, 2])\n",
    "\n",
    "        return z[-1]\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        N = np.random.randint(1, self.N + 1)\n",
    "        u, v, z, x = self._initialize_tensors(x, N)\n",
    "\n",
    "        for t in range(N - 1):\n",
    "            u = tf.tensor_scatter_nd_update(u, [[t + 1]], [\n",
    "                tf.nn.relu(self.a[0] * u[t] - self.a[1] * v[t] + self.a[2] * z[t] + self.a[3] * x)])\n",
    "            v = tf.tensor_scatter_nd_update(v, [[t + 1]], [\n",
    "                tf.nn.relu(self.a[6] * z[t] - self.a[4] * u[t] - self.a[5] * v[t] + self.a[7] * x)])\n",
    "            w = tf.nn.relu(x)\n",
    "            z = tf.tensor_scatter_nd_update(z, [[t + 1]], [(v[t + 1] - u[t + 1]) * tf.math.exp(-self.K * tf.math.pow(x, 2)) + w])\n",
    "\n",
    "        if tf.rank(x) == 4 and tf.shape(x)[1] > 1:\n",
    "            z = tf.transpose(z[-1], [0, 3, 1, 2])\n",
    "\n",
    "        return z[-1]\n",
    "\n",
    "    def LeakyReLU(self, x, alpha=0.1):\n",
    "        N = np.random.randint(1, self.N + 1)\n",
    "        u, v, z, x = self._initialize_tensors(x, N)\n",
    "\n",
    "        for t in range(N - 1):\n",
    "            u = tf.tensor_scatter_nd_update(u, [[t + 1]], [\n",
    "                tf.nn.leaky_relu(self.a[0] * u[t] - self.a[1] * v[t] + self.a[2] * z[t] + self.a[3] * x, alpha=alpha)])\n",
    "            v = tf.tensor_scatter_nd_update(v, [[t + 1]], [\n",
    "                tf.nn.leaky_relu(self.a[6] * z[t] - self.a[4] * u[t] - self.a[5] * v[t] + self.a[7] * x, alpha=alpha)])\n",
    "            w = tf.nn.leaky_relu(x, alpha=alpha)\n",
    "            z = tf.tensor_scatter_nd_update(z, [[t + 1]], [(v[t + 1] - u[t + 1]) * tf.math.exp(-self.K * tf.math.pow(x, 2)) + w])\n",
    "\n",
    "        if tf.rank(x) == 4 and tf.shape(x)[1] > 1:\n",
    "            z = tf.transpose(z[-1], [0, 3, 1, 2])\n",
    "\n",
    "        return z[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义激活层\n",
    "class LeeOscillatorTanhLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LeeOscillatorTanhLayer, self).__init__(**kwargs)\n",
    "        self.oscillator = LeeOscillator()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.oscillator.Tanh(inputs)\n",
    "\n",
    "class LeeOscillatorSigmoidLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LeeOscillatorSigmoidLayer, self).__init__(**kwargs)\n",
    "        self.oscillator = LeeOscillator()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.oscillator.Sigmoid(inputs)\n",
    "\n",
    "class LeeOscillatorReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LeeOscillatorReLULayer, self).__init__(**kwargs)\n",
    "        self.oscillator = LeeOscillator()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.oscillator.ReLU(inputs)\n",
    "\n",
    "class LeeOscillatorLeakyReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LeeOscillatorLeakyReLULayer, self).__init__(**kwargs)\n",
    "        self.oscillator = LeeOscillator()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.oscillator.LeakyReLU(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/20\n",
      "8982/8982 [==============================] - 54s 6ms/step - loss: 2.3329 - accuracy: 0.4098 - val_loss: 1.9027 - val_accuracy: 0.5058\n",
      "Epoch 2/20\n",
      "8982/8982 [==============================] - 47s 5ms/step - loss: 1.8158 - accuracy: 0.5254 - val_loss: 1.7016 - val_accuracy: 0.5663\n",
      "Epoch 3/20\n",
      "8982/8982 [==============================] - 46s 5ms/step - loss: 1.6121 - accuracy: 0.5838 - val_loss: 1.8314 - val_accuracy: 0.5445\n",
      "Epoch 4/20\n",
      "8982/8982 [==============================] - 48s 5ms/step - loss: 1.5034 - accuracy: 0.6133 - val_loss: 1.6043 - val_accuracy: 0.5984\n",
      "Epoch 5/20\n",
      "8982/8982 [==============================] - 46s 5ms/step - loss: 1.3966 - accuracy: 0.6323 - val_loss: 1.5041 - val_accuracy: 0.6180\n",
      "Epoch 6/20\n",
      "8982/8982 [==============================] - 46s 5ms/step - loss: 1.3255 - accuracy: 0.6485 - val_loss: 1.4362 - val_accuracy: 0.6322\n",
      "Epoch 7/20\n",
      "8982/8982 [==============================] - 46s 5ms/step - loss: 1.2621 - accuracy: 0.6700 - val_loss: 1.3874 - val_accuracy: 0.6465\n",
      "Epoch 8/20\n",
      "8982/8982 [==============================] - 49s 5ms/step - loss: 1.2016 - accuracy: 0.6808 - val_loss: 1.4447 - val_accuracy: 0.6354\n",
      "Epoch 9/20\n",
      "8982/8982 [==============================] - 46s 5ms/step - loss: 1.1494 - accuracy: 0.6992 - val_loss: 1.3462 - val_accuracy: 0.6674\n",
      "Epoch 10/20\n",
      "8982/8982 [==============================] - 45s 5ms/step - loss: 1.0997 - accuracy: 0.7099 - val_loss: 1.3326 - val_accuracy: 0.6736\n",
      "Epoch 11/20\n",
      "8982/8982 [==============================] - 45s 5ms/step - loss: 1.0439 - accuracy: 0.7211 - val_loss: 1.2971 - val_accuracy: 0.6799\n",
      "Epoch 12/20\n",
      "8982/8982 [==============================] - 47s 5ms/step - loss: 0.9912 - accuracy: 0.7329 - val_loss: 1.3791 - val_accuracy: 0.6585\n",
      "Epoch 13/20\n",
      "8982/8982 [==============================] - 46s 5ms/step - loss: 0.9389 - accuracy: 0.7486 - val_loss: 1.2634 - val_accuracy: 0.6999\n",
      "Epoch 14/20\n",
      "8982/8982 [==============================] - 45s 5ms/step - loss: 0.9011 - accuracy: 0.7602 - val_loss: 1.3405 - val_accuracy: 0.6915\n",
      "Epoch 15/20\n",
      "8982/8982 [==============================] - 45s 5ms/step - loss: 0.8501 - accuracy: 0.7688 - val_loss: 1.3208 - val_accuracy: 0.6959\n",
      "Epoch 16/20\n",
      "8982/8982 [==============================] - 46s 5ms/step - loss: 0.8072 - accuracy: 0.7816 - val_loss: 1.3046 - val_accuracy: 0.7039\n",
      "Epoch 17/20\n",
      "8982/8982 [==============================] - 46s 5ms/step - loss: 0.7655 - accuracy: 0.7916 - val_loss: 1.4342 - val_accuracy: 0.6968\n",
      "Epoch 18/20\n",
      "8982/8982 [==============================] - 45s 5ms/step - loss: 0.7283 - accuracy: 0.8006 - val_loss: 1.3487 - val_accuracy: 0.7084\n",
      "Epoch 19/20\n",
      "8982/8982 [==============================] - 45s 5ms/step - loss: 0.6908 - accuracy: 0.8094 - val_loss: 1.3940 - val_accuracy: 0.7079\n",
      "Epoch 20/20\n",
      "8982/8982 [==============================] - 46s 5ms/step - loss: 0.6672 - accuracy: 0.8195 - val_loss: 1.4174 - val_accuracy: 0.7070\n",
      "8982/8982 [==============================] - 6s 637us/step\n",
      "Training Set: \n",
      "accuracy: 85.34%\n",
      "2246/2246 [==============================] - 1s 618us/step\n",
      "Test Set: \n",
      "accuracy: 70.70%\n"
     ]
    }
   ],
   "source": [
    "# Define the model with Embedding layer\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, 128, input_length=max_len, name='embedding_1'))\n",
    "model.add(SpatialDropout1D(0.2, name='spatial_dropout1d_1'))\n",
    "\n",
    "model.add(Conv1D(32, 5, padding='same', name='conv1d_1'))\n",
    "#model.add(LeeOscillatorReLULayer())\n",
    "model.add(Activation(\"relu\", name='activation_1'))\n",
    "model.add(MaxPooling1D(pool_size=2, name='max_pooling1d_1'))\n",
    "\n",
    "model.add(Conv1D(64, 3, padding='same', name='conv1d_2'))\n",
    "#model.add(LeeOscillatorReLULayer())\n",
    "model.add(Activation(\"relu\", name='activation_2'))\n",
    "model.add(MaxPooling1D(pool_size=2, name='max_pooling1d_2'))\n",
    "\n",
    "model.add(Flatten(name='flatten_1'))\n",
    "model.add(Dense(128, name='dense_1'))\n",
    "#model.add(Activation(\"relu\", name='activation_3'))\n",
    "model.add(LeeOscillatorReLULayer())\n",
    "model.add(Dropout(0.2, name='dropout_1'))\n",
    "model.add(Dense(50, name='dense_2'))\n",
    "#model.add(LeeOscillatorReLULayer())\n",
    "model.add(Activation(\"relu\", name='activation_4'))\n",
    "model.add(Dropout(0.2, name='dropout_2'))\n",
    "model.add(Dense(46, name='dense_3'))  # Output layer with 2 units (binary classification)\n",
    "model.add(Activation(\"softmax\", name='activation_5'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, oh_y_train,\n",
    "          batch_size=64,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, oh_y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model.evaluate(X_train, oh_y_train)\n",
    "print(\"Training Set:\", \"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "scores = model.evaluate(X_test, oh_y_test)\n",
    "print(\"Test Set:\", \"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the sequences back to words\n",
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = {v: k for k, v in word_index.items()}\n",
    "decoded_X_train = [' '.join([reverse_word_index.get(i - 3, '?') for i in seq]) for seq in X_train]\n",
    "decoded_X_test = [' '.join([reverse_word_index.get(i - 3, '?') for i in seq]) for seq in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "knn_X_train = vectorizer.fit_transform(decoded_X_train)\n",
    "knn_X_test = vectorizer.transform(decoded_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='brute', n_neighbors=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train final k-NN\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=1, algorithm=\"brute\")\n",
    "knn_clf.fit(knn_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Accuracy Test: 0.7537845057880677\n"
     ]
    }
   ],
   "source": [
    "# Check the accuracy on this particular split to make sure that it is not too far removed from k-fold.\n",
    "knn_predictions_test = knn_clf.predict(knn_X_test)\n",
    "print(\"k-NN Accuracy Test:\", accuracy_score(y_test, knn_predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  1,  0, ...,  0,  0,  0],\n",
       "       [ 0, 77,  1, ...,  0,  0,  0],\n",
       "       [ 0,  1, 13, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  6,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  4,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  1]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check confusion matrix kNN\n",
    "confusion_matrix(y_test, knn_predictions_test, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  1,  0, ...,  0,  0,  0],\n",
       "       [ 0, 74,  0, ...,  0,  0,  0],\n",
       "       [ 0,  7,  4, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  3, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check confusion matrix NN\n",
    "confusion_matrix(y_test, model.predict_classes(X_test), labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the CBR model to disk\n",
    "pickle.dump(knn_clf, open('k-nn_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Keras Models to disk\n",
    "model.save(\"NN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Dataframes\n",
    "np.save(\"X_train\", X_train)\n",
    "np.save(\"X_test\", X_test)\n",
    "np.save(\"y_train\", y_train)\n",
    "np.save(\"y_test\", y_test)\n",
    "\n",
    "np.save(\"knn_X_train\", knn_X_train)\n",
    "np.save(\"knn_X_test\", knn_X_test)\n",
    "\n",
    "np.save(\"oh_y_train\", oh_y_train)\n",
    "np.save(\"oh_y_test\", oh_y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
