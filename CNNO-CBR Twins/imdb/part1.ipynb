{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Conv1D, MaxPooling1D, Flatten, Dropout, Activation, Layer\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bill\\.conda\\envs\\python3.6\\lib\\site-packages\\keras\\datasets\\imdb.py:101: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "c:\\Users\\Bill\\.conda\\envs\\python3.6\\lib\\site-packages\\keras\\datasets\\imdb.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (25000, 500)\n",
      "25000 train samples\n",
      "25000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDB dataset\n",
    "max_features = 5000  # Number of words to consider as features\n",
    "max_len = 500  # Cut texts after this number of words (among top max_features most common words)\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Pad sequences (to ensure equal length of all sequences)\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# One-hot encode the labels\n",
    "oh_y_train = to_categorical(y_train, 2)\n",
    "oh_y_test = to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeeOscillator:\n",
    "    def __init__(self, a=[1, 1, 1, 1, -1, -1, -1, -1], b=[0.6, 0.6, -0.5, 0.5, -0.6, -0.6, -0.5, 0.5], K=500, N=100):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "\n",
    "    def _initialize_tensors(self, x, N):\n",
    "        shape = tf.shape(x)\n",
    "        rank = tf.rank(x)\n",
    "\n",
    "        if rank == 2:\n",
    "            x = tf.expand_dims(tf.expand_dims(x, axis=1), axis=1)\n",
    "            shape = tf.shape(x)\n",
    "        elif rank == 4 and shape[1] > 1:\n",
    "            x = tf.transpose(x, [0, 2, 3, 1])\n",
    "            shape = tf.shape(x)\n",
    "\n",
    "        init_shape = tf.concat([[N], shape], axis=0)\n",
    "        u = tf.zeros(init_shape, dtype=tf.float32)\n",
    "        v = tf.zeros(init_shape, dtype=tf.float32)\n",
    "        z = tf.zeros(init_shape, dtype=tf.float32)\n",
    "        u = tf.tensor_scatter_nd_update(u, [[0]], [u[0] + 0.2])\n",
    "        z = tf.tensor_scatter_nd_update(z, [[0]], [z[0] + 0.2])\n",
    "\n",
    "        return u, v, z, x\n",
    "\n",
    "    def Tanh(self, x):\n",
    "        N = np.random.randint(1, self.N + 1)\n",
    "        u, v, z, x = self._initialize_tensors(x, N)\n",
    "\n",
    "        for t in range(N - 1):\n",
    "            u = tf.tensor_scatter_nd_update(u, [[t + 1]], [\n",
    "                tf.math.tanh(self.a[0] * u[t] - self.a[1] * v[t] + self.a[2] * z[t] + self.a[3] * x)])\n",
    "            v = tf.tensor_scatter_nd_update(v, [[t + 1]], [\n",
    "                tf.math.tanh(self.a[6] * z[t] - self.a[4] * u[t] - self.a[5] * v[t] + self.a[7] * x)])\n",
    "            w = tf.math.tanh(x)\n",
    "            z = tf.tensor_scatter_nd_update(z, [[t + 1]], [(v[t + 1] - u[t + 1]) * tf.math.exp(-self.K * tf.math.pow(x, 2)) + w])\n",
    "\n",
    "        if tf.rank(x) == 4 and tf.shape(x)[1] > 1:\n",
    "            z = tf.transpose(z[-1], [0, 3, 1, 2])\n",
    "\n",
    "        return z[-1]\n",
    "\n",
    "    def Sigmoid(self, x):\n",
    "        N = np.random.randint(1, self.N + 1)\n",
    "        u, v, z, x = self._initialize_tensors(x, N)\n",
    "\n",
    "        for t in range(N - 1):\n",
    "            u = tf.tensor_scatter_nd_update(u, [[t + 1]], [\n",
    "                tf.math.sigmoid(self.b[0] * u[t] - self.b[1] * v[t] + self.b[2] * z[t] + self.b[3] * x)])\n",
    "            v = tf.tensor_scatter_nd_update(v, [[t + 1]], [\n",
    "                tf.math.sigmoid(self.b[6] * z[t] - self.b[4] * u[t] - self.b[5] * v[t] + self.b[7] * x)])\n",
    "            w = tf.math.sigmoid(x)\n",
    "            z = tf.tensor_scatter_nd_update(z, [[t + 1]], [(v[t + 1] - u[t + 1]) * tf.math.exp(-self.K * tf.math.pow(x, 2)) + w])\n",
    "\n",
    "        if tf.rank(x) == 4 and tf.shape(x)[1] > 1:\n",
    "            z = tf.transpose(z[-1], [0, 3, 1, 2])\n",
    "\n",
    "        return z[-1]\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        N = np.random.randint(1, self.N + 1)\n",
    "        u, v, z, x = self._initialize_tensors(x, N)\n",
    "\n",
    "        for t in range(N - 1):\n",
    "            u = tf.tensor_scatter_nd_update(u, [[t + 1]], [\n",
    "                tf.nn.relu(self.a[0] * u[t] - self.a[1] * v[t] + self.a[2] * z[t] + self.a[3] * x)])\n",
    "            v = tf.tensor_scatter_nd_update(v, [[t + 1]], [\n",
    "                tf.nn.relu(self.a[6] * z[t] - self.a[4] * u[t] - self.a[5] * v[t] + self.a[7] * x)])\n",
    "            w = tf.nn.relu(x)\n",
    "            z = tf.tensor_scatter_nd_update(z, [[t + 1]], [(v[t + 1] - u[t + 1]) * tf.math.exp(-self.K * tf.math.pow(x, 2)) + w])\n",
    "\n",
    "        if tf.rank(x) == 4 and tf.shape(x)[1] > 1:\n",
    "            z = tf.transpose(z[-1], [0, 3, 1, 2])\n",
    "\n",
    "        return z[-1]\n",
    "\n",
    "    def LeakyReLU(self, x, alpha=0.1):\n",
    "        N = np.random.randint(1, self.N + 1)\n",
    "        u, v, z, x = self._initialize_tensors(x, N)\n",
    "\n",
    "        for t in range(N - 1):\n",
    "            u = tf.tensor_scatter_nd_update(u, [[t + 1]], [\n",
    "                tf.nn.leaky_relu(self.a[0] * u[t] - self.a[1] * v[t] + self.a[2] * z[t] + self.a[3] * x, alpha=alpha)])\n",
    "            v = tf.tensor_scatter_nd_update(v, [[t + 1]], [\n",
    "                tf.nn.leaky_relu(self.a[6] * z[t] - self.a[4] * u[t] - self.a[5] * v[t] + self.a[7] * x, alpha=alpha)])\n",
    "            w = tf.nn.leaky_relu(x, alpha=alpha)\n",
    "            z = tf.tensor_scatter_nd_update(z, [[t + 1]], [(v[t + 1] - u[t + 1]) * tf.math.exp(-self.K * tf.math.pow(x, 2)) + w])\n",
    "\n",
    "        if tf.rank(x) == 4 and tf.shape(x)[1] > 1:\n",
    "            z = tf.transpose(z[-1], [0, 3, 1, 2])\n",
    "\n",
    "        return z[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义激活层\n",
    "class LeeOscillatorTanhLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LeeOscillatorTanhLayer, self).__init__(**kwargs)\n",
    "        self.oscillator = LeeOscillator()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.oscillator.Tanh(inputs)\n",
    "\n",
    "class LeeOscillatorSigmoidLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LeeOscillatorSigmoidLayer, self).__init__(**kwargs)\n",
    "        self.oscillator = LeeOscillator()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.oscillator.Sigmoid(inputs)\n",
    "\n",
    "class LeeOscillatorReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LeeOscillatorReLULayer, self).__init__(**kwargs)\n",
    "        self.oscillator = LeeOscillator()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.oscillator.ReLU(inputs)\n",
    "\n",
    "class LeeOscillatorLeakyReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LeeOscillatorLeakyReLULayer, self).__init__(**kwargs)\n",
    "        self.oscillator = LeeOscillator()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.oscillator.LeakyReLU(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Bill\\.conda\\envs\\python3.6\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From c:\\Users\\Bill\\.conda\\envs\\python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Bill\\.conda\\envs\\python3.6\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From c:\\Users\\Bill\\.conda\\envs\\python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 99s 4ms/step - loss: 0.6937 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5140\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 95s 4ms/step - loss: 0.6928 - accuracy: 0.5121 - val_loss: 0.6917 - val_accuracy: 0.5194\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 97s 4ms/step - loss: 0.6871 - accuracy: 0.5471 - val_loss: 0.6679 - val_accuracy: 0.6101\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 90s 4ms/step - loss: 0.6168 - accuracy: 0.6700 - val_loss: 0.4995 - val_accuracy: 0.7771\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.4929 - accuracy: 0.7702 - val_loss: 0.5437 - val_accuracy: 0.7240\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.3958 - accuracy: 0.8258 - val_loss: 0.3508 - val_accuracy: 0.8444\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 90s 4ms/step - loss: 0.3356 - accuracy: 0.8563 - val_loss: 0.3245 - val_accuracy: 0.8570\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.3047 - accuracy: 0.8732 - val_loss: 0.3678 - val_accuracy: 0.8420\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 90s 4ms/step - loss: 0.2674 - accuracy: 0.8920 - val_loss: 0.3824 - val_accuracy: 0.8363\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.2569 - accuracy: 0.8975 - val_loss: 0.2652 - val_accuracy: 0.8892\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 90s 4ms/step - loss: 0.2362 - accuracy: 0.9067 - val_loss: 0.2725 - val_accuracy: 0.8855\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.2199 - accuracy: 0.9142 - val_loss: 0.2533 - val_accuracy: 0.8948\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 95s 4ms/step - loss: 0.2037 - accuracy: 0.9211 - val_loss: 0.2870 - val_accuracy: 0.8810\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.1925 - accuracy: 0.9271 - val_loss: 0.2575 - val_accuracy: 0.8959\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.1766 - accuracy: 0.9336 - val_loss: 0.3319 - val_accuracy: 0.8676\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.1747 - accuracy: 0.9332 - val_loss: 0.2647 - val_accuracy: 0.8976\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 94s 4ms/step - loss: 0.1599 - accuracy: 0.9393 - val_loss: 0.3239 - val_accuracy: 0.8710\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.1520 - accuracy: 0.9440 - val_loss: 0.2733 - val_accuracy: 0.8926\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.1394 - accuracy: 0.9500 - val_loss: 0.2795 - val_accuracy: 0.8960\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.1338 - accuracy: 0.9507 - val_loss: 0.2852 - val_accuracy: 0.8953\n",
      "25000/25000 [==============================] - 11s 444us/step\n",
      "Training Set: \n",
      "accuracy: 96.94%\n",
      "25000/25000 [==============================] - 11s 438us/step\n",
      "Test Set: \n",
      "accuracy: 89.53%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the model with Embedding layer\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, 128, input_length=max_len, name='embedding_1'))\n",
    "model.add(SpatialDropout1D(0.2, name='spatial_dropout1d_1'))\n",
    "\n",
    "model.add(Conv1D(32, 5, padding='same', name='conv1d_1'))\n",
    "model.add(Activation(\"relu\", name='activation_1'))\n",
    "model.add(MaxPooling1D(pool_size=2, name='max_pooling1d_1'))\n",
    "\n",
    "model.add(Conv1D(64, 3, padding='same', name='conv1d_2'))\n",
    "model.add(Activation(\"relu\", name='activation_2'))\n",
    "model.add(MaxPooling1D(pool_size=2, name='max_pooling1d_2'))\n",
    "\n",
    "model.add(Flatten(name='flatten_1'))\n",
    "model.add(Dense(128, name='dense_1'))\n",
    "#model.add(Activation(\"relu\", name='activation_3'))\n",
    "model.add(LeeOscillatorReLULayer())\n",
    "model.add(Dropout(0.2, name='dropout_1'))\n",
    "model.add(Dense(50, name='dense_2'))\n",
    "model.add(Activation(\"relu\", name='activation_4'))\n",
    "model.add(Dropout(0.2, name='dropout_2'))\n",
    "model.add(Dense(2, name='dense_3'))  # Output layer with 2 units (binary classification)\n",
    "model.add(Activation(\"sigmoid\", name='activation_5'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, oh_y_train,\n",
    "          batch_size=256,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, oh_y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model.evaluate(X_train, oh_y_train)\n",
    "print(\"Training Set:\", \"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "scores = model.evaluate(X_test, oh_y_test)\n",
    "print(\"Test Set:\", \"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the sequences back to words\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {v: k for k, v in word_index.items()}\n",
    "decoded_X_train = [' '.join([reverse_word_index.get(i - 3, '?') for i in seq]) for seq in X_train]\n",
    "decoded_X_test = [' '.join([reverse_word_index.get(i - 3, '?') for i in seq]) for seq in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "knn_X_train = vectorizer.fit_transform(decoded_X_train)\n",
    "knn_X_test = vectorizer.transform(decoded_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='brute', n_neighbors=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train final k-NN\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=1, algorithm=\"brute\")\n",
    "knn_clf.fit(knn_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Accuracy Test: 0.63316\n"
     ]
    }
   ],
   "source": [
    "# Check the accuracy on this particular split to make sure that it is not too far removed from k-fold.\n",
    "knn_predictions_test = knn_clf.predict(knn_X_test)\n",
    "print(\"k-NN Accuracy Test:\", accuracy_score(y_test, knn_predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8092, 4408],\n",
       "       [4763, 7737]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check confusion matrix kNN\n",
    "confusion_matrix(y_test, knn_predictions_test, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11190,  1310],\n",
       "       [ 1295, 11205]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check confusion matrix NN\n",
    "confusion_matrix(y_test, model.predict_classes(X_test), labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the CBR model to disk\n",
    "pickle.dump(knn_clf, open('k-nn_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Keras Models to disk\n",
    "model.save(\"NN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Dataframes\n",
    "np.save(\"X_train\", X_train)\n",
    "np.save(\"X_test\", X_test)\n",
    "np.save(\"y_train\", y_train)\n",
    "np.save(\"y_test\", y_test)\n",
    "\n",
    "np.save(\"knn_X_train\", knn_X_train)\n",
    "np.save(\"knn_X_test\", knn_X_test)\n",
    "\n",
    "np.save(\"oh_y_train\", oh_y_train)\n",
    "np.save(\"oh_y_test\", oh_y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
