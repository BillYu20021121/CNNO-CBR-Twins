{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Conv1D, MaxPooling1D, Flatten, Dropout, Activation, Layer\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (6336, 500)\n",
      "6336 train samples\n",
      "1584 test samples\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "file_path = 'sentiment_analysis.csv'\n",
    "sentiment_df = pd.read_csv(file_path)\n",
    "\n",
    "# 提取文本和标签\n",
    "texts = sentiment_df['tweet'].astype(str).tolist()  # 将推文列转换为字符串列表\n",
    "labels = sentiment_df['label'].tolist()\n",
    "\n",
    "# 设置参数\n",
    "max_features = 5000  # 考虑的最大词汇数量\n",
    "max_len = 500  # 填充后的序列最大长度\n",
    "\n",
    "# 创建分词器并拟合数据\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# 将文本转换为整数索引的序列\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# 填充序列\n",
    "X = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# 将标签转换为numpy数组并进行独热编码\n",
    "y = to_categorical(labels, 2)\n",
    "\n",
    "# 将数据分割为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 独热编码的标签\n",
    "oh_y_train = y_train\n",
    "oh_y_test = y_test\n",
    "\n",
    "# 打印形状信息\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeeOscillator:\n",
    "    def __init__(self, a=[1, 1, 1, 1, -1, -1, -1, -1], b=[0.6, 0.6, -0.5, 0.5, -0.6, -0.6, -0.5, 0.5], K=500, N=100):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "\n",
    "    def _initialize_tensors(self, x, N):\n",
    "        shape = tf.shape(x)\n",
    "        rank = tf.rank(x)\n",
    "\n",
    "        if rank == 2:\n",
    "            x = tf.expand_dims(tf.expand_dims(x, axis=1), axis=1)\n",
    "            shape = tf.shape(x)\n",
    "        elif rank == 4 and shape[1] > 1:\n",
    "            x = tf.transpose(x, [0, 2, 3, 1])\n",
    "            shape = tf.shape(x)\n",
    "\n",
    "        init_shape = tf.concat([[N], shape], axis=0)\n",
    "        u = tf.zeros(init_shape, dtype=tf.float32)\n",
    "        v = tf.zeros(init_shape, dtype=tf.float32)\n",
    "        z = tf.zeros(init_shape, dtype=tf.float32)\n",
    "        u = tf.tensor_scatter_nd_update(u, [[0]], [u[0] + 0.2])\n",
    "        z = tf.tensor_scatter_nd_update(z, [[0]], [z[0] + 0.2])\n",
    "\n",
    "        return u, v, z, x\n",
    "\n",
    "    def Tanh(self, x):\n",
    "        N = np.random.randint(1, self.N + 1)\n",
    "        u, v, z, x = self._initialize_tensors(x, N)\n",
    "\n",
    "        for t in range(N - 1):\n",
    "            u = tf.tensor_scatter_nd_update(u, [[t + 1]], [\n",
    "                tf.math.tanh(self.a[0] * u[t] - self.a[1] * v[t] + self.a[2] * z[t] + self.a[3] * x)])\n",
    "            v = tf.tensor_scatter_nd_update(v, [[t + 1]], [\n",
    "                tf.math.tanh(self.a[6] * z[t] - self.a[4] * u[t] - self.a[5] * v[t] + self.a[7] * x)])\n",
    "            w = tf.math.tanh(x)\n",
    "            z = tf.tensor_scatter_nd_update(z, [[t + 1]], [(v[t + 1] - u[t + 1]) * tf.math.exp(-self.K * tf.math.pow(x, 2)) + w])\n",
    "\n",
    "        if tf.rank(x) == 4 and tf.shape(x)[1] > 1:\n",
    "            z = tf.transpose(z[-1], [0, 3, 1, 2])\n",
    "\n",
    "        return z[-1]\n",
    "\n",
    "    def Sigmoid(self, x):\n",
    "        N = np.random.randint(1, self.N + 1)\n",
    "        u, v, z, x = self._initialize_tensors(x, N)\n",
    "\n",
    "        for t in range(N - 1):\n",
    "            u = tf.tensor_scatter_nd_update(u, [[t + 1]], [\n",
    "                tf.math.sigmoid(self.b[0] * u[t] - self.b[1] * v[t] + self.b[2] * z[t] + self.b[3] * x)])\n",
    "            v = tf.tensor_scatter_nd_update(v, [[t + 1]], [\n",
    "                tf.math.sigmoid(self.b[6] * z[t] - self.b[4] * u[t] - self.b[5] * v[t] + self.b[7] * x)])\n",
    "            w = tf.math.sigmoid(x)\n",
    "            z = tf.tensor_scatter_nd_update(z, [[t + 1]], [(v[t + 1] - u[t + 1]) * tf.math.exp(-self.K * tf.math.pow(x, 2)) + w])\n",
    "\n",
    "        if tf.rank(x) == 4 and tf.shape(x)[1] > 1:\n",
    "            z = tf.transpose(z[-1], [0, 3, 1, 2])\n",
    "\n",
    "        return z[-1]\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        N = np.random.randint(1, self.N + 1)\n",
    "        u, v, z, x = self._initialize_tensors(x, N)\n",
    "\n",
    "        for t in range(N - 1):\n",
    "            u = tf.tensor_scatter_nd_update(u, [[t + 1]], [\n",
    "                tf.nn.relu(self.a[0] * u[t] - self.a[1] * v[t] + self.a[2] * z[t] + self.a[3] * x)])\n",
    "            v = tf.tensor_scatter_nd_update(v, [[t + 1]], [\n",
    "                tf.nn.relu(self.a[6] * z[t] - self.a[4] * u[t] - self.a[5] * v[t] + self.a[7] * x)])\n",
    "            w = tf.nn.relu(x)\n",
    "            z = tf.tensor_scatter_nd_update(z, [[t + 1]], [(v[t + 1] - u[t + 1]) * tf.math.exp(-self.K * tf.math.pow(x, 2)) + w])\n",
    "\n",
    "        if tf.rank(x) == 4 and tf.shape(x)[1] > 1:\n",
    "            z = tf.transpose(z[-1], [0, 3, 1, 2])\n",
    "\n",
    "        return z[-1]\n",
    "\n",
    "    def LeakyReLU(self, x, alpha=0.1):\n",
    "        N = np.random.randint(1, self.N + 1)\n",
    "        u, v, z, x = self._initialize_tensors(x, N)\n",
    "\n",
    "        for t in range(N - 1):\n",
    "            u = tf.tensor_scatter_nd_update(u, [[t + 1]], [\n",
    "                tf.nn.leaky_relu(self.a[0] * u[t] - self.a[1] * v[t] + self.a[2] * z[t] + self.a[3] * x, alpha=alpha)])\n",
    "            v = tf.tensor_scatter_nd_update(v, [[t + 1]], [\n",
    "                tf.nn.leaky_relu(self.a[6] * z[t] - self.a[4] * u[t] - self.a[5] * v[t] + self.a[7] * x, alpha=alpha)])\n",
    "            w = tf.nn.leaky_relu(x, alpha=alpha)\n",
    "            z = tf.tensor_scatter_nd_update(z, [[t + 1]], [(v[t + 1] - u[t + 1]) * tf.math.exp(-self.K * tf.math.pow(x, 2)) + w])\n",
    "\n",
    "        if tf.rank(x) == 4 and tf.shape(x)[1] > 1:\n",
    "            z = tf.transpose(z[-1], [0, 3, 1, 2])\n",
    "\n",
    "        return z[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义激活层\n",
    "class LeeOscillatorTanhLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LeeOscillatorTanhLayer, self).__init__(**kwargs)\n",
    "        self.oscillator = LeeOscillator()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.oscillator.Tanh(inputs)\n",
    "\n",
    "class LeeOscillatorSigmoidLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LeeOscillatorSigmoidLayer, self).__init__(**kwargs)\n",
    "        self.oscillator = LeeOscillator()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.oscillator.Sigmoid(inputs)\n",
    "\n",
    "class LeeOscillatorReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LeeOscillatorReLULayer, self).__init__(**kwargs)\n",
    "        self.oscillator = LeeOscillator()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.oscillator.ReLU(inputs)\n",
    "\n",
    "class LeeOscillatorLeakyReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LeeOscillatorLeakyReLULayer, self).__init__(**kwargs)\n",
    "        self.oscillator = LeeOscillator()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.oscillator.LeakyReLU(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6336 samples, validate on 1584 samples\n",
      "Epoch 1/20\n",
      "6336/6336 [==============================] - 153s 24ms/step - loss: 0.5944 - accuracy: 0.7446 - val_loss: 0.5883 - val_accuracy: 0.7273\n",
      "Epoch 2/20\n",
      "6336/6336 [==============================] - 135s 21ms/step - loss: 0.5692 - accuracy: 0.7484 - val_loss: 0.5886 - val_accuracy: 0.7273\n",
      "Epoch 3/20\n",
      "6336/6336 [==============================] - 138s 22ms/step - loss: 0.5689 - accuracy: 0.7484 - val_loss: 0.5848 - val_accuracy: 0.7273\n",
      "Epoch 4/20\n",
      "6336/6336 [==============================] - 137s 22ms/step - loss: 0.5631 - accuracy: 0.7484 - val_loss: 0.5651 - val_accuracy: 0.7273\n",
      "Epoch 5/20\n",
      "6336/6336 [==============================] - 136s 22ms/step - loss: 0.4716 - accuracy: 0.7495 - val_loss: 0.3877 - val_accuracy: 0.7273\n",
      "Epoch 6/20\n",
      "6336/6336 [==============================] - 140s 22ms/step - loss: 0.3534 - accuracy: 0.8357 - val_loss: 0.3251 - val_accuracy: 0.8706\n",
      "Epoch 7/20\n",
      "6336/6336 [==============================] - 143s 23ms/step - loss: 0.3113 - accuracy: 0.8633 - val_loss: 0.3036 - val_accuracy: 0.8750\n",
      "Epoch 8/20\n",
      "6336/6336 [==============================] - 137s 22ms/step - loss: 0.2984 - accuracy: 0.8720 - val_loss: 0.3107 - val_accuracy: 0.8731\n",
      "Epoch 9/20\n",
      "6336/6336 [==============================] - 137s 22ms/step - loss: 0.2907 - accuracy: 0.8736 - val_loss: 0.2973 - val_accuracy: 0.8819\n",
      "Epoch 10/20\n",
      "6336/6336 [==============================] - 138s 22ms/step - loss: 0.2670 - accuracy: 0.8867 - val_loss: 0.2799 - val_accuracy: 0.8769\n",
      "Epoch 11/20\n",
      "6336/6336 [==============================] - 141s 22ms/step - loss: 0.2518 - accuracy: 0.8936 - val_loss: 0.2912 - val_accuracy: 0.8794\n",
      "Epoch 12/20\n",
      "6336/6336 [==============================] - 149s 24ms/step - loss: 0.2431 - accuracy: 0.8999 - val_loss: 0.2746 - val_accuracy: 0.8845\n",
      "Epoch 13/20\n",
      "6336/6336 [==============================] - 152s 24ms/step - loss: 0.2277 - accuracy: 0.9096 - val_loss: 0.2685 - val_accuracy: 0.8845\n",
      "Epoch 14/20\n",
      "6336/6336 [==============================] - 144s 23ms/step - loss: 0.2144 - accuracy: 0.9140 - val_loss: 0.2740 - val_accuracy: 0.8813\n",
      "Epoch 15/20\n",
      "6336/6336 [==============================] - 146s 23ms/step - loss: 0.2099 - accuracy: 0.9160 - val_loss: 0.2730 - val_accuracy: 0.8832\n",
      "Epoch 16/20\n",
      "6336/6336 [==============================] - 143s 23ms/step - loss: 0.1968 - accuracy: 0.9220 - val_loss: 0.2779 - val_accuracy: 0.8801\n",
      "Epoch 17/20\n",
      "6336/6336 [==============================] - 142s 22ms/step - loss: 0.1845 - accuracy: 0.9287 - val_loss: 0.2701 - val_accuracy: 0.8889\n",
      "Epoch 18/20\n",
      "6336/6336 [==============================] - 138s 22ms/step - loss: 0.1738 - accuracy: 0.9342 - val_loss: 0.2778 - val_accuracy: 0.8826\n",
      "Epoch 19/20\n",
      "6336/6336 [==============================] - 137s 22ms/step - loss: 0.1633 - accuracy: 0.9381 - val_loss: 0.2819 - val_accuracy: 0.8826\n",
      "Epoch 20/20\n",
      "6336/6336 [==============================] - 135s 21ms/step - loss: 0.1537 - accuracy: 0.9454 - val_loss: 0.2853 - val_accuracy: 0.8883\n",
      "6336/6336 [==============================] - 9s 1ms/step\n",
      "Training Set: \n",
      "accuracy: 95.71%\n",
      "1584/1584 [==============================] - 2s 1ms/step\n",
      "Test Set: \n",
      "accuracy: 88.83%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the model with Embedding layer\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, 128, input_length=max_len, name='embedding_1'))\n",
    "model.add(SpatialDropout1D(0.2, name='spatial_dropout1d_1'))\n",
    "\n",
    "model.add(Conv1D(32, 5, padding='same', name='conv1d_1'))\n",
    "model.add(Activation(\"relu\", name='activation_1'))\n",
    "model.add(MaxPooling1D(pool_size=2, name='max_pooling1d_1'))\n",
    "\n",
    "model.add(Conv1D(64, 3, padding='same', name='conv1d_2'))\n",
    "model.add(Activation(\"relu\", name='activation_2'))\n",
    "model.add(MaxPooling1D(pool_size=2, name='max_pooling1d_2'))\n",
    "\n",
    "model.add(Flatten(name='flatten_1'))\n",
    "model.add(Dense(128, name='dense_1'))\n",
    "#model.add(Activation(\"relu\", name='activation_3'))\n",
    "model.add(LeeOscillatorReLULayer())\n",
    "model.add(Dropout(0.2, name='dropout_1'))\n",
    "model.add(Dense(50, name='dense_2'))\n",
    "model.add(Activation(\"relu\", name='activation_4'))\n",
    "model.add(Dropout(0.2, name='dropout_2'))\n",
    "model.add(Dense(2, name='dense_3'))  # Output layer with 2 units (binary classification)\n",
    "model.add(Activation(\"sigmoid\", name='activation_5'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, oh_y_train,\n",
    "          batch_size=256,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, oh_y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model.evaluate(X_train, oh_y_train)\n",
    "print(\"Training Set:\", \"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "scores = model.evaluate(X_test, oh_y_test)\n",
    "print(\"Test Set:\", \"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the sequences back to words\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {v: k for k, v in word_index.items()}\n",
    "decoded_X_train = [' '.join([reverse_word_index.get(i - 3, '?') for i in seq]) for seq in X_train]\n",
    "decoded_X_test = [' '.join([reverse_word_index.get(i - 3, '?') for i in seq]) for seq in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "knn_X_train = vectorizer.fit_transform(decoded_X_train)\n",
    "knn_X_test = vectorizer.transform(decoded_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='brute', n_neighbors=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train final k-NN\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=1, algorithm=\"brute\")\n",
    "knn_clf.fit(knn_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Accuracy Test: 0.8301767676767676\n"
     ]
    }
   ],
   "source": [
    "# Check the accuracy on this particular split to make sure that it is not too far removed from k-fold.\n",
    "knn_predictions_test = knn_clf.predict(knn_X_test)\n",
    "print(\"k-NN Accuracy Test:\", accuracy_score(y_test, knn_predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1048  104]\n",
      " [ 165  267]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 将独热编码的标签转换回原始标签\n",
    "y_test_original = np.argmax(y_test, axis=1)\n",
    "knn_predictions_test_original = np.argmax(knn_predictions_test, axis=1)\n",
    "\n",
    "# 计算混淆矩阵\n",
    "cm = confusion_matrix(y_test_original, knn_predictions_test_original)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1057   95]\n",
      " [  81  351]]\n"
     ]
    }
   ],
   "source": [
    "# 将独热编码的标签转换回原始标签\n",
    "y_test_original = np.argmax(y_test, axis=1)\n",
    "# 获取模型预测的标签\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "# 计算混淆矩阵\n",
    "cm = confusion_matrix(y_test_original, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the CBR model to disk\n",
    "pickle.dump(knn_clf, open('k-nn_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Keras Models to disk\n",
    "model.save(\"NN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Dataframes\n",
    "np.save(\"X_train\", X_train)\n",
    "np.save(\"X_test\", X_test)\n",
    "np.save(\"y_train\", y_train)\n",
    "np.save(\"y_test\", y_test)\n",
    "\n",
    "np.save(\"knn_X_train\", knn_X_train)\n",
    "np.save(\"knn_X_test\", knn_X_test)\n",
    "\n",
    "np.save(\"oh_y_train\", oh_y_train)\n",
    "np.save(\"oh_y_test\", oh_y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
